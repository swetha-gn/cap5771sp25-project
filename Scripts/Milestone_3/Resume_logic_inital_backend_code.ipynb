{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkdtSeOI8_OI"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Please ensure you have imported a Gemini API key from AI Studio.\n",
        "You can do this directly in the Secrets tab on the left.\n",
        "\n",
        "After doing so, please run the setup cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HIJ1q928_OK"
      },
      "source": [
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N0Kwaq-8_OL"
      },
      "source": [
        "# Generated Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mMZrUsdC8_OL",
        "outputId": "12aa6184-822c-4465-e2a7-76da72861275"
      },
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyDTo5J7P130C5cUbqwcUADHtb6MXj1_2ms\")\n",
        "\n",
        "# Create the model\n",
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 65536,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "  model_name = \"models/gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        ")\n",
        "\n",
        "chat_session = model.start_chat(\n",
        "  history=[\n",
        "  ]\n",
        ")\n",
        "\n",
        "response = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
        "\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide me with the input! I need some text or a question to be able to respond.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "SECTION_HEADERS = {\n",
        "    'summary': ['summary', 'objective', 'profile', 'about'],\n",
        "    'skills': ['skills', 'technical skills', 'key skills'],\n",
        "    'work_experience': ['experience', 'work history', 'professional background', 'employment'],\n",
        "    'education': ['education', 'academic background', 'qualifications'],\n",
        "    'certifications': ['certifications', 'licenses', 'courses'],\n",
        "    'projects': ['projects', 'personal projects', 'academic projects'],\n",
        "    'achievements': ['achievements', 'awards', 'recognition'],\n",
        "}\n",
        "\n",
        "def extract_sections(text):\n",
        "    lines = text.split('\\n')\n",
        "    sections = {}\n",
        "    current_section = None\n",
        "    buffer = []\n",
        "\n",
        "    def match_section(line):\n",
        "        for key, variants in SECTION_HEADERS.items():\n",
        "            for variant in variants:\n",
        "                if re.match(rf\"^\\s*{variant}\\s*$\", line.strip(), re.IGNORECASE):\n",
        "                    return key\n",
        "        return None\n",
        "\n",
        "    for line in lines:\n",
        "        section = match_section(line)\n",
        "        if section:\n",
        "            if current_section and buffer:\n",
        "                sections[current_section] = '\\n'.join(buffer).strip()\n",
        "                buffer = []\n",
        "            current_section = section\n",
        "        elif current_section:\n",
        "            buffer.append(line)\n",
        "\n",
        "    if current_section and buffer:\n",
        "        sections[current_section] = '\\n'.join(buffer).strip()\n",
        "\n",
        "    return sections\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    import pdfplumber\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        text = '\\n'.join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "    return text\n",
        "\n",
        "def clean_token(token):\n",
        "    token = re.sub(r\"[^a-zA-Z0-9#+\\-./]\", \"\", token)\n",
        "    token = token.strip().lower()\n",
        "    return token if token and len(token) > 1 and not token.isdigit() else None\n",
        "\n",
        "def expand_slashes(tokens):\n",
        "    expanded = set()\n",
        "    for token in tokens:\n",
        "        if \"/\" in token:\n",
        "            expanded.update(token.split(\"/\"))\n",
        "        else:\n",
        "            expanded.add(token)\n",
        "    return expanded\n",
        "\n",
        "def extract_skills_from_section(skills_text):\n",
        "    skills = set()\n",
        "    lines = skills_text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        line = line.lower()\n",
        "        if \":\" in line:\n",
        "            line = line.split(\":\", 1)[1]  # remove heading like \"Architectures:\"\n",
        "        line = re.sub(r\"[()\\[\\]]\", \"\", line)\n",
        "        tokens = re.split(r\",|\\.|/| and | or | with |;|\\n\", line)\n",
        "        for token in tokens:\n",
        "            clean = clean_token(token)\n",
        "            if clean:\n",
        "                skills.add(clean)\n",
        "    return sorted(expand_slashes(skills))\n",
        "\n",
        "# Sample driver test\n",
        "if __name__ == '__main__':\n",
        "    path = \"/content/drive/MyDrive/IDS/Updated_resume.pdf\"\n",
        "    text = extract_text_from_pdf(path)\n",
        "    sections = extract_sections(text)\n",
        "    print(\"\\n===== SKILLS SECTION TEXT =====\\n\")\n",
        "    print(sections.get(\"skills\", \"<No skills section detected>\"))\n",
        "\n",
        "    print(\"\\n===== CLEANED SKILLS ARRAY =====\\n\")\n",
        "    extracted = extract_skills_from_section(sections.get(\"skills\", \"\"))\n",
        "    print(extracted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTY6Bu3b4znf",
        "outputId": "fc485a49-9f79-4fd4-87ff-a2176f727c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SKILLS SECTION TEXT =====\n",
            "\n",
            "• Python libraries & frameworks: Pandas, NumPy, Keras, ScikitLearn, MatPlotlib, Pytorch, TensorFlow, JAX, DataPrep.\n",
            "• Architectures: CNN (ResNet18/50, VGG16), RNN (LSTM, Attention Mechanisms), Transformer Models (Llama, BERT), RAG.\n",
            "• Data Engineering & Big Data: SQL, NoSQL (MongoDB, Firebase). Feature Engineering, Data Cleaning (DataPrep, Pandas, PySpark).\n",
            "• Programming & Software Engineering: Python, Java, C, JavaScript, Git, Flutter.\n",
            "• Fine-tuning & Optimization: LoRA/QLoRA fine-tuning, (1,2,4,6,8) bit quantization, beam search decoding, Model compression,\n",
            "knowledge distillation.\n",
            "WORK EXPERIENCE\n",
            "Researcher – Florida Semiconductor Institute, University of Florida. October 2024 - Present\n",
            "• Pioneered photonic neural network development in Dr. Volker Sorger Lab through cross-institutional collaboration with UC Davis,\n",
            "implementing EIC-PIC integrated systems using PyTorch and JAX for AI accelerator compatibility.\n",
            "• Modified LeNet-5 architecture via 6-bit weight/activation quantization and constrained design (1 conv-layer with positive weights, 6\n",
            "optimized kernels, 1 separable conv-layer, avg-pool, 2 dense layers), achieving 98.3% inference accuracy on MNIST/CIFAR-10 through\n",
            "JAX-powered optimizations.\n",
            "• Implemented precision batch processing with layer-specific quantization strategies, enhancing AI accelerator compatibility while\n",
            "maintaining ~ 10 TOPS/W consumption through hardware-aware model compression techniques.\n",
            "Machine Learning Engineer - Optum Global Solutions, India. Jan 2024 – July 2024\n",
            "• Led 7-member team developing clinical chatbots using RAG/Hugging Face with spaCy NER, achieving 99.2% intent accuracy and 35%\n",
            "cost reduction via LoRA fine-tuning.\n",
            "• Engineered medical data pipeline integrating PyMuPDF for clinical document parsing and BERT embeddings for HER context\n",
            "extraction, achieving 40% faster data retrieval through PyTorch-optimized similarity search across 12M+ medical records.\n",
            "• Architected ICD-10/CPT code prediction system using XGBoost and custom RAG framework, enhancing context-aware medical coding\n",
            "accuracy to 92.8% (18% improvement vs. previous benchmarks) via hybrid LLM-NLP integration with hospital EMR system\n",
            "Undergraduate Researcher – PSG College of Technology. July 2023 – Oct 2023\n",
            "• Designed and implemented LLM-driven solutions using OpenAI GPT models, Lang Chain framework, and Python libraries (PyTorch,\n",
            "NumPy) to automate academic literature reviews under the mentorship of Dr. Lovelyn Rose S.\n",
            "• Developed multi-stage NLP pipeline integrating text summarization (OpenAI LLMs), vector embeddings (FAISS similarity search),\n",
            "arXiv API for paper retrieval, and PyMuPDF for PDF text extraction, achieving 55% reduction in manual effort for literature synthesis.\n",
            "• Fine-tuned document processing through section-specific summarization with customized prompting strategies for\n",
            "abstracts/methodologies/results, leveraging Lang Chain’s document loaders and chain-of-thought prompting to improve information\n",
            "retention by 32%.\n",
            "Project Intern – Ugam Solutions June 2022 – Dec 2022\n",
            "• Spearheaded cross-functional 5-member team in developing computer vision system using ResNet-50 + LSTM hybrid architecture with\n",
            "spatial attention mechanisms, reducing hallucinated image descriptors by 22% versus CNN-LSTM baseline through attention gate\n",
            "optimization.\n",
            "• Optimized model deployment via beam search decoding and CIDEr score-driven training (96.3% accuracy), achieving 10% of SOTA\n",
            "computational costs through model compression techniques and hardware-aware quantization strategies.\n",
            "• Performed architecture benchmarking across VGG16, Inception-v3, and GRU encoder-decoder frameworks, identifying optimal ResNet-\n",
            "LSTM configuration through gradient-based attention visualization, reducing training cycles by 18% via selective layer initialization.\n",
            "\n",
            "===== CLEANED SKILLS ARRAY =====\n",
            "\n",
            "['1separableconv-layer', '2denselayers', '2intentaccuracy', '3accuracy', '3inferenceaccuracyonmnist', '818improvementvs', '8bitquantization', 'abstracts', 'accuracyto92', 'achieving10ofsota', 'achieving40fasterdataretrievalthroughpytorch-optimizedsimilaritysearchacross12m+medicalrecords', 'achieving55reductioninmanualeffortforliteraturesynthesis', 'achieving98', 'achieving99', 'activationquantization', 'architectedicd-10', 'arxivapiforpaperretrieval', 'attentionmechanisms', 'avg-pool', 'beamsearchdecoding', 'bert', 'bertembeddingsforhercontext', 'chain-of-thoughtpromptingtoimproveinformation', 'ciderscore-driventraining96', 'cifar-10through', 'cnnresnet18', 'computationalcoststhroughmodelcompressiontechniques', 'constraineddesign1conv-layer', 'costreductionvialorafine-tuning', 'cptcodepredictionsystemusingxgboost', 'customizedpromptingstrategiesfor', 'customragframework', 'datacleaningdataprep', 'dataprep', 'designed', 'developedmulti-stagenlppipelineintegratingtextsummarizationopenaillms', 'engineeredmedicaldatapipelineintegratingpymupdfforclinicaldocumentparsing', 'enhancingaiacceleratorcompatibilitywhile', 'enhancingcontext-awaremedicalcoding', 'extraction', 'featureengineering', 'fine-tuneddocumentprocessingthroughsection-specificsummarization', 'firebase', 'flutter', 'git', 'gruencoder-decoderframeworks', 'hardware-awarequantizationstrategies', 'hospitalemrsystem', 'huggingface', 'identifyingoptimalresnet-', 'implementedllm-drivensolutionsusingopenaigptmodels', 'implementedprecisionbatchprocessing', 'implementingeic-picintegratedsystemsusingpytorch', 'inception-v3', 'india', 'jan2024july2024', 'java', 'javascript', 'jax', 'jax-poweredoptimizations', 'jaxforaiacceleratorcompatibility', 'july2023oct2023', 'keras', 'knowledgedistillation', 'langchainframework', 'layer-specificquantizationstrategies', 'led7-memberteamdevelopingclinicalchatbotsusingrag', 'leveraginglangchainsdocumentloaders', 'lora', 'lovelynroses', 'lstmconfigurationthroughgradient-basedattentionvisualization', 'machinelearningengineer-optumglobalsolutions', 'maintaining10tops', 'matplotlib', 'methodologies', 'modelcompression', 'modifiedlenet-5architecturevia6-bitweight', 'nosqlmongodb', 'numpy', 'numpytoautomateacademicliteraturereviewsunderthementorshipofdr', 'october2024-present', 'optimization', 'optimizedkernels', 'optimizedmodeldeploymentviabeamsearchdecoding', 'pandas', 'performedarchitecturebenchmarkingacrossvgg16', 'pioneeredphotonicneuralnetworkdevelopmentindr', 'positiveweights', 'previousbenchmarksviahybridllm-nlpintegration', 'projectinternugamsolutionsjune2022dec2022', 'pymupdfforpdftextextraction', 'pyspark', 'python', 'pythonlibrariespytorch', 'pytorch', 'qlorafine-tuning', 'rag', 'reducinghallucinatedimagedescriptorsby22versuscnn-lstmbaselinethroughattentiongate', 'reducingtrainingcyclesby18viaselectivelayerinitialization', 'researcherfloridasemiconductorinstitute', 'results', 'retentionby32', 'rnnlstm', 'scikitlearn', 'spacyner', 'spatialattentionmechanisms', 'spearheadedcross-functional5-memberteamindevelopingcomputervisionsystemusingresnet-50+lstmhybridarchitecturewith', 'sql', 'tensorflow', 'transformermodelsllama', 'ucdavis', 'undergraduateresearcherpsgcollegeoftechnology', 'universityofflorida', 'vectorembeddingsfaisssimilaritysearch', 'vgg16', 'volkersorgerlabthroughcross-institutionalcollaboration', 'wconsumptionthroughhardware-awaremodelcompressiontechniques', 'workexperience']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pdfplumber\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        text = ' '.join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "    return text\n",
        "\n",
        "# Define common resume section headers\n",
        "SECTION_HEADERS = {\n",
        "    'summary': ['summary', 'objective', 'profile', 'about'],\n",
        "    'skills': ['skills', 'technical skills', 'key skills'],\n",
        "    'work_experience': ['experience', 'work history', 'professional background', 'employment'],\n",
        "    'education': ['education', 'academic background', 'qualifications'],\n",
        "    'certifications': ['certifications', 'licenses', 'courses'],\n",
        "    'projects': ['projects', 'personal projects', 'academic projects'],\n",
        "    'achievements': ['achievements', 'awards', 'recognition'],\n",
        "}\n",
        "\n",
        "# Function to extract sections from resume text\n",
        "def extract_sections(text):\n",
        "    lines = text.split('\\n')\n",
        "    sections = {}\n",
        "    current_section = None\n",
        "    buffer = []\n",
        "\n",
        "    def match_section(line):\n",
        "        for key, variants in SECTION_HEADERS.items():\n",
        "            for variant in variants:\n",
        "                if re.match(rf\"^\\s*{variant}\\s*[:\\-]*\\s*$\", line.strip(), re.IGNORECASE):\n",
        "                    return key\n",
        "        return None\n",
        "\n",
        "    for line in lines:\n",
        "        section = match_section(line)\n",
        "        if section:\n",
        "            if current_section and buffer:\n",
        "                sections[current_section] = '\\n'.join(buffer).strip()\n",
        "                buffer = []\n",
        "            current_section = section\n",
        "        elif current_section:\n",
        "            buffer.append(line)\n",
        "\n",
        "    # Final section\n",
        "    if current_section and buffer:\n",
        "        sections[current_section] = '\\n'.join(buffer).strip()\n",
        "\n",
        "    return sections\n",
        "\n",
        "# Function to clean and extract skills from the 'skills' section\n",
        "def extract_skills_from_section(skills_text):\n",
        "    skills = set()\n",
        "    lines = skills_text.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        # Remove label before colon (e.g., \"Architectures:\")\n",
        "        if ':' in line:\n",
        "            line = line.split(':', 1)[1]\n",
        "\n",
        "        # Normalize line\n",
        "        line = line.lower().strip()\n",
        "        line = re.sub(r\"[^a-z0-9,/.+()\\s\\-]\", \"\", line)\n",
        "\n",
        "        # Expand slashes like ResNet18/50\n",
        "        line = re.sub(r\"(\\w+)/(\\w+)\", r\"\\1, \\2\", line)\n",
        "\n",
        "        # Tokenize on comma, slash, dot-space, etc.\n",
        "        tokens = re.split(r\",|\\s+and\\s+|\\s+or\\s+|\\s*/\\s*|\\.\\s*\", line)\n",
        "\n",
        "        for token in tokens:\n",
        "            token = token.strip().replace(\".\", \"\")\n",
        "            if len(token) > 1 and not token.isdigit():\n",
        "                skills.add(token)\n",
        "\n",
        "    return skills\n",
        "\n",
        "# Driver test\n",
        "path = \"/content/drive/MyDrive/IDS/Updated_resume.pdf\"\n",
        "resume_text = extract_text_from_pdf(path)\n",
        "sections = extract_sections(resume_text)\n",
        "\n",
        "print(\"\\n===== SKILLS SECTION TEXT =====\\n\")\n",
        "print(sections.get(\"skills\", \"<No skills section detected>\"))\n",
        "\n",
        "print(\"\\n===== CLEANED SKILLS ARRAY =====\\n\")\n",
        "extracted = extract_skills_from_section(sections.get(\"skills\", \"\"))\n",
        "print(sorted(extracted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXjbuAMb6zgI",
        "outputId": "4c575cdb-04df-4ea6-8353-496fcffaa9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SKILLS SECTION TEXT =====\n",
            "\n",
            "• Python libraries & frameworks: Pandas, NumPy, Keras, ScikitLearn, MatPlotlib, Pytorch, TensorFlow, JAX, DataPrep.\n",
            "• Architectures: CNN (ResNet18/50, VGG16), RNN (LSTM, Attention Mechanisms), Transformer Models (Llama, BERT), RAG.\n",
            "• Data Engineering & Big Data: SQL, NoSQL (MongoDB, Firebase). Feature Engineering, Data Cleaning (DataPrep, Pandas, PySpark).\n",
            "• Programming & Software Engineering: Python, Java, C, JavaScript, Git, Flutter.\n",
            "• Fine-tuning & Optimization: LoRA/QLoRA fine-tuning, (1,2,4,6,8) bit quantization, beam search decoding, Model compression,\n",
            "knowledge distillation.\n",
            "WORK EXPERIENCE\n",
            "Researcher – Florida Semiconductor Institute, University of Florida. October 2024 - Present\n",
            "• Pioneered photonic neural network development in Dr. Volker Sorger Lab through cross-institutional collaboration with UC Davis,\n",
            "implementing EIC-PIC integrated systems using PyTorch and JAX for AI accelerator compatibility.\n",
            "• Modified LeNet-5 architecture via 6-bit weight/activation quantization and constrained design (1 conv-layer with positive weights, 6\n",
            "optimized kernels, 1 separable conv-layer, avg-pool, 2 dense layers), achieving 98.3% inference accuracy on MNIST/CIFAR-10 through\n",
            "JAX-powered optimizations.\n",
            "• Implemented precision batch processing with layer-specific quantization strategies, enhancing AI accelerator compatibility while\n",
            "maintaining ~ 10 TOPS/W consumption through hardware-aware model compression techniques.\n",
            "Machine Learning Engineer - Optum Global Solutions, India. Jan 2024 – July 2024\n",
            "• Led 7-member team developing clinical chatbots using RAG/Hugging Face with spaCy NER, achieving 99.2% intent accuracy and 35%\n",
            "cost reduction via LoRA fine-tuning.\n",
            "• Engineered medical data pipeline integrating PyMuPDF for clinical document parsing and BERT embeddings for HER context\n",
            "extraction, achieving 40% faster data retrieval through PyTorch-optimized similarity search across 12M+ medical records.\n",
            "• Architected ICD-10/CPT code prediction system using XGBoost and custom RAG framework, enhancing context-aware medical coding\n",
            "accuracy to 92.8% (18% improvement vs. previous benchmarks) via hybrid LLM-NLP integration with hospital EMR system\n",
            "Undergraduate Researcher – PSG College of Technology. July 2023 – Oct 2023\n",
            "• Designed and implemented LLM-driven solutions using OpenAI GPT models, Lang Chain framework, and Python libraries (PyTorch,\n",
            "NumPy) to automate academic literature reviews under the mentorship of Dr. Lovelyn Rose S.\n",
            "• Developed multi-stage NLP pipeline integrating text summarization (OpenAI LLMs), vector embeddings (FAISS similarity search),\n",
            "arXiv API for paper retrieval, and PyMuPDF for PDF text extraction, achieving 55% reduction in manual effort for literature synthesis.\n",
            "• Fine-tuned document processing through section-specific summarization with customized prompting strategies for\n",
            "abstracts/methodologies/results, leveraging Lang Chain’s document loaders and chain-of-thought prompting to improve information\n",
            "retention by 32%.\n",
            "Project Intern – Ugam Solutions June 2022 – Dec 2022\n",
            "• Spearheaded cross-functional 5-member team in developing computer vision system using ResNet-50 + LSTM hybrid architecture with\n",
            "spatial attention mechanisms, reducing hallucinated image descriptors by 22% versus CNN-LSTM baseline through attention gate\n",
            "optimization.\n",
            "• Optimized model deployment via beam search decoding and CIDEr score-driven training (96.3% accuracy), achieving 10% of SOTA\n",
            "computational costs through model compression techniques and hardware-aware quantization strategies.\n",
            "• Performed architecture benchmarking across VGG16, Inception-v3, and GRU encoder-decoder frameworks, identifying optimal ResNet-\n",
            "LSTM configuration through gradient-based attention visualization, reducing training cycles by 18% via selective layer initialization.\n",
            "\n",
            "===== CLEANED SKILLS ARRAY =====\n",
            "\n",
            "['(1', '1 separable conv-layer', '2 dense layers)', '2 intent accuracy', '3 accuracy)', '3 inference accuracy on mnist', '8 (18 improvement vs', '8) bit quantization', 'abstracts', 'accuracy to 92', 'achieving 10 of sota', 'achieving 40 faster data retrieval through pytorch-optimized similarity search across 12m+ medical records', 'achieving 55 reduction in manual effort for literature synthesis', 'achieving 98', 'achieving 99', 'activation quantization', 'architected icd-10', 'arxiv api for paper retrieval', 'attention mechanisms)', 'avg-pool', 'beam search decoding', 'bert embeddings for her context', 'bert)', 'chain-of-thought prompting to improve information', 'cider score-driven training (96', 'cifar-10 through', 'cnn (resnet18', 'computational costs through model compression techniques', 'constrained design (1 conv-layer with positive weights', 'cost reduction via lora fine-tuning', 'cpt code prediction system using xgboost', 'custom rag framework', 'data cleaning (dataprep', 'dataprep', 'designed', 'developed multi-stage nlp pipeline integrating text summarization (openai llms)', 'engineered medical data pipeline integrating pymupdf for clinical document parsing', 'enhancing ai accelerator compatibility while', 'enhancing context-aware medical coding', 'extraction', 'feature engineering', 'fine-tuned document processing through section-specific summarization with customized prompting strategies for', 'firebase)', 'flutter', 'git', 'gru encoder-decoder frameworks', 'hardware-aware quantization strategies', 'hugging face with spacy ner', 'identifying optimal resnet-', 'implemented llm-driven solutions using openai gpt models', 'implemented precision batch processing with layer-specific quantization strategies', 'implementing eic-pic integrated systems using pytorch', 'inception-v3', 'india', 'jan 2024  july 2024', 'java', 'javascript', 'jax', 'jax for ai accelerator compatibility', 'jax-powered optimizations', 'july 2023  oct 2023', 'keras', 'knowledge distillation', 'lang chain framework', 'led 7-member team developing clinical chatbots using rag', 'leveraging lang chains document loaders', 'lora', 'lovelyn rose s', 'lstm configuration through gradient-based attention visualization', 'machine learning engineer - optum global solutions', 'maintaining  10 tops', 'matplotlib', 'methodologies', 'model compression', 'modified lenet-5 architecture via 6-bit weight', 'nosql (mongodb', 'numpy', 'numpy) to automate academic literature reviews under the mentorship of dr', 'october 2024 - present', 'optimization', 'optimized kernels', 'optimized model deployment via beam search decoding', 'pandas', 'performed architecture benchmarking across vgg16', 'pioneered photonic neural network development in dr', 'previous benchmarks) via hybrid llm-nlp integration with hospital emr system', 'project intern  ugam solutions june 2022  dec 2022', 'pymupdf for pdf text extraction', 'pyspark)', 'python', 'python libraries (pytorch', 'pytorch', 'qlora fine-tuning', 'rag', 'reducing hallucinated image descriptors by 22 versus cnn-lstm baseline through attention gate', 'reducing training cycles by 18 via selective layer initialization', 'researcher  florida semiconductor institute', 'results', 'retention by 32', 'rnn (lstm', 'scikitlearn', 'spatial attention mechanisms', 'spearheaded cross-functional 5-member team in developing computer vision system using resnet-50 + lstm hybrid architecture with', 'sql', 'tensorflow', 'transformer models (llama', 'undergraduate researcher  psg college of technology', 'university of florida', 'vector embeddings (faiss similarity search)', 'vgg16)', 'volker sorger lab through cross-institutional collaboration with uc davis', 'w consumption through hardware-aware model compression techniques', 'work experience']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=\"AIzaSyDTo5J7P130C5cUbqwcUADHtb6MXj1_2ms\")\n",
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
        "\n",
        "# Section headers for parsing\n",
        "SECTION_HEADERS = {\n",
        "    'summary': ['summary', 'objective', 'profile', 'about'],\n",
        "    'skills': ['skills', 'technical skills', 'key skills', 'technical summary', 'highlights'],\n",
        "    'work_experience': ['experience', 'work history', 'professional background', 'employment'],\n",
        "    'education': ['education', 'academic background', 'qualifications'],\n",
        "    'certifications': ['certifications', 'licenses', 'courses'],\n",
        "    'projects': ['projects', 'personal projects', 'academic projects'],\n",
        "    'achievements': ['achievements', 'awards', 'recognition'],\n",
        "}\n",
        "\n",
        "# Resume parsing\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        text = ' '.join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "    return text\n",
        "\n",
        "def extract_sections(text):\n",
        "    lines = text.split('\\n')\n",
        "    sections = {}\n",
        "    current_section = None\n",
        "    buffer = []\n",
        "\n",
        "    def match_section(line):\n",
        "        for key, variants in SECTION_HEADERS.items():\n",
        "            for variant in variants:\n",
        "                if re.match(rf\"^\\s*{variant}\\s*$\", line.strip(), re.IGNORECASE):\n",
        "                    return key\n",
        "        return None\n",
        "\n",
        "    for line in lines:\n",
        "        section = match_section(line)\n",
        "        if section:\n",
        "            if current_section and buffer:\n",
        "                sections[current_section] = '\\n'.join(buffer).strip()\n",
        "                buffer = []\n",
        "            current_section = section\n",
        "        elif current_section:\n",
        "            buffer.append(line)\n",
        "    if current_section and buffer:\n",
        "        sections[current_section] = '\\n'.join(buffer).strip()\n",
        "    return sections\n",
        "\n",
        "# Skill extraction\n",
        "def extract_skills_from_text(text):\n",
        "    skills = set()\n",
        "    lines = text.splitlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if \":\" in line:\n",
        "            line = line.split(\":\")[1]\n",
        "        line = re.sub(r\"[^\\w\\s,()/+.-]\", \"\", line)\n",
        "        tokens = re.split(r\",| and | or | with \", line)\n",
        "        for token in tokens:\n",
        "            token = token.strip()\n",
        "            if len(token) > 1 and not token.isdigit():\n",
        "                skills.add(token.lower())\n",
        "    return skills\n",
        "\n",
        "def clean_and_flatten_skills(raw_skills):\n",
        "    cleaned = set()\n",
        "    for skill in raw_skills:\n",
        "        skill = skill.strip().lower().rstrip('.),;:-')\n",
        "        slash_parts = re.split(r\"[\\/]+\", skill)\n",
        "        base = slash_parts[0].rstrip('0123456789')\n",
        "        if len(slash_parts) > 1:\n",
        "            for part in slash_parts[1:]:\n",
        "                expanded = base + part if base else part\n",
        "                cleaned.add(expanded.strip())\n",
        "        else:\n",
        "            paren_match = re.match(r\"(.*?)\\(([^)]+)\\)\", skill)\n",
        "            if paren_match:\n",
        "                base = paren_match.group(1).strip()\n",
        "                nested = paren_match.group(2)\n",
        "                if base:\n",
        "                    cleaned.add(base)\n",
        "                for item in re.split(r\"[,/|]\", nested):\n",
        "                    item = item.strip()\n",
        "                    if len(item) > 1:\n",
        "                        cleaned.add(item)\n",
        "            else:\n",
        "                for part in re.split(r\"[.;:]+\", skill):\n",
        "                    part = part.strip()\n",
        "                    if len(part) > 1:\n",
        "                        cleaned.add(part)\n",
        "    return cleaned\n",
        "\n",
        "# Core matching pipeline\n",
        "def run_formatted_pipeline():\n",
        "    name = input(\"What's your name? \")\n",
        "    current_role = input(\"What's your current role? \")\n",
        "    desired_role = input(\"What role do you want to transition into? \")\n",
        "\n",
        "    salary_rows = df_jd[df_jd['Role'].str.contains(desired_role, case=False, na=False)][['Role', 'Average Salary']].head(3)\n",
        "    print(\"\\nPredicted Salary Ranges:\")\n",
        "    print(salary_rows)\n",
        "\n",
        "    job_suggestions = df_jd[df_jd['Role'].str.contains(desired_role, case=False, na=False)][['Job Title', 'Company', 'Country']].head(3)\n",
        "    print(\"\\nAvailable Jobs:\")\n",
        "    print(job_suggestions)\n",
        "\n",
        "    if current_role.lower() != desired_role.lower() or current_role.lower() in ['student', 'intern']:\n",
        "        print(\"\\nYou’re new or switching fields. Here are some courses to get you started:\")\n",
        "        df_cc['combined'] = df_cc['title'] + ' ' + df_cc['Skills']\n",
        "        courses = df_cc[df_cc['combined'].str.contains(desired_role.split()[0], case=False, na=False)][['title', 'URL']].head(3)\n",
        "        print(courses)\n",
        "        return\n",
        "\n",
        "    resume_path = input(\"\\n📄 Upload your resume (path in Colab): \")\n",
        "    resume_text = extract_text_from_pdf(resume_path)\n",
        "    parsed = extract_sections(resume_text)\n",
        "\n",
        "    print(\"\\n===== SKILLS SECTION TEXT =====\")\n",
        "    print(parsed.get(\"skills\", \"<No skills section detected>\"))\n",
        "\n",
        "    raw_skills = extract_skills_from_text(parsed.get(\"skills\", \"\"))\n",
        "    resume_skills = clean_and_flatten_skills(raw_skills)\n",
        "    print(\"\\n===== CLEANED SKILLS ARRAY =====\")\n",
        "    print(sorted(resume_skills))\n",
        "\n",
        "    role_match = df_rs[df_rs['position_title'].str.lower().str.contains(desired_role.lower())]\n",
        "    if role_match.empty:\n",
        "        print(\"No matching role found.\")\n",
        "        return\n",
        "\n",
        "    required_skills_text = role_match.iloc[0]['Required Skills']\n",
        "    required_skills = set([s.strip().lower() for s in re.split(r'[.,\\n\\-\\u2022\\|]', str(required_skills_text)) if len(s.strip()) > 1])\n",
        "    matched_skills = set()\n",
        "\n",
        "    for skill in resume_skills:\n",
        "        for req in required_skills:\n",
        "            if re.search(rf\"\\b{re.escape(skill)}\\b\", req.lower()):\n",
        "                matched_skills.add(skill)\n",
        "                break\n",
        "\n",
        "    missing_skills = resume_skills - matched_skills\n",
        "\n",
        "    if resume_skills:\n",
        "        skill_score = round((len(matched_skills) / len(resume_skills)) * 100, 2)\n",
        "    else:\n",
        "        skill_score = 0.0\n",
        "\n",
        "    print(\"Matched Skills (from resume):\", matched_skills)\n",
        "    print(\"Missing Skills (from resume):\", missing_skills)\n",
        "    print(f\"Skill Match Score: {skill_score}%\")\n",
        "\n",
        "    if skill_score < 50:\n",
        "        print(\"\\nYou're missing some key skills. Here are Coursera courses:\")\n",
        "        df_cc['combined_text'] = df_cc['title'].astype(str) + ' ' + df_cc['Skills'].astype(str)\n",
        "        matches = df_cc[df_cc['combined_text'].apply(lambda x: any(ms in x.lower() for ms in missing_skills))][['title', 'URL']].head(3)\n",
        "        print(matches)\n",
        "\n",
        "        print(\"\\nConnect with professionals:\")\n",
        "        pros = df_ll[df_ll['Current_Role'].str.contains(desired_role.split()[0], case=False, na=False)][['Full_Name', 'Contact_mail']].dropna().head(3)\n",
        "        print(pros)\n",
        "    else:\n",
        "        prompt = f\"{name} is applying for a {desired_role} job and has relevant skills: {', '.join(matched_skills)}. Suggest a cold outreach email.\"\n",
        "        print(\"\\nCold Mail Template:\")\n",
        "        print(model.generate_content(prompt).text)\n",
        "\n",
        "        print(\"\\nJobs Available:\")\n",
        "        print(job_suggestions)\n",
        "\n",
        "        print(\"\\nRecruiters in the field:\")\n",
        "        pros = df_ll[df_ll['Current_Role'].str.contains(desired_role.split()[0], case=False, na=False)][['Full_Name', 'Contact_mail']].dropna().head(3)\n",
        "        print(pros)\n",
        "\n",
        "\n",
        "run_formatted_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw14sDkzLVQi",
        "outputId": "1f267120-bd23-4b64-e446-5e174f215a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👋 What's your name? Swetha\n",
            "💼 What's your current role? Web Developer\n",
            "🎯 What role do you want to transition into? Web Developer\n",
            "\n",
            "💰 Predicted Salary Ranges:\n",
            "                       Role  Average Salary\n",
            "1    Frontend Web Developer        0.466081\n",
            "185   Backend Web Developer        1.196646\n",
            "453  Frontend Web Developer       -0.463731\n",
            "\n",
            "📌 Available Jobs:\n",
            "         Job Title                       Company       Country\n",
            "1    Web Developer  PNC Financial Services Group  Turkmenistan\n",
            "185  Web Developer                          AGCO     St. Lucia\n",
            "453  Web Developer                     BMW Group       Finland\n",
            "\n",
            "📄 Upload your resume (path in Colab): /content/tmpy8xbjya8.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SKILLS SECTION TEXT =====\n",
            "Python, JavaScript, React, Node.js, HTML, CSS, Git, SQL, MongoDB, REST APIs, Flask, Agile, Docker\n",
            "\n",
            "===== CLEANED SKILLS ARRAY =====\n",
            "['agile', 'css', 'docker', 'flask', 'git', 'html', 'javascript', 'js', 'mongodb', 'node', 'python', 'react', 'rest apis', 'sql']\n",
            "✅ Matched Skills (from resume): {'rest apis', 'css', 'html', 'javascript'}\n",
            "❌ Missing Skills (from resume): {'mongodb', 'agile', 'js', 'python', 'git', 'sql', 'flask', 'docker', 'react', 'node'}\n",
            "📊 Skill Match Score: 28.57%\n",
            "\n",
            "📘 You're missing some key skills. Here are Coursera courses:\n",
            "                                        title  \\\n",
            "7   Foundations of Music Promotion & Branding   \n",
            "21    Electronic Music Performance Techniques   \n",
            "37        The Structured Query Language (SQL)   \n",
            "\n",
            "                                                  URL  \n",
            "7   https://www.coursera.org/learn/foundations-of-...  \n",
            "21  https://www.coursera.org/learn/edi-performance...  \n",
            "37  https://www.coursera.org/learn/the-structured-...  \n",
            "\n",
            "👥 Connect with professionals:\n",
            "             Full_Name                  Contact_mail\n",
            "1420  Lars Klingenberg  lars.klingenberg@outlook.com\n",
            "2668   Oskar Bjornsson   oskar.bjornsson@outlook.com\n",
            "2924   Gunnar Claesson   gunnar.claesson@outlook.com\n"
          ]
        }
      ]
    }
  ]
}